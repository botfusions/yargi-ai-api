import os
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
import httpx
import openai
import uvicorn
from typing import Optional

app = FastAPI(
    title="YargÄ± AI API", 
    description="OpenRouter LLM-powered Turkish Legal Search API",
    version="1.0.0"
)

# CORS - frontend baÄŸlantÄ±sÄ± iÃ§in
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… GeÃ§miÅŸte Ã§alÄ±ÅŸan port strategy
PORT = int(os.getenv("PORT", 8001))
HOST = os.getenv("HOST", "0.0.0.0")

# ğŸ”‘ OpenRouter Configuration
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_MODEL = os.getenv("OPENROUTER_MODEL", "openai/gpt-oss-120b")

# OpenRouter client setup
openrouter_client = openai.OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

@app.get("/health")
async def health():
    """
    Sistem durumu kontrolÃ¼
    """
    return {
        "status": "healthy",
        "service": "yargi-ai-api",
        "port": PORT,
        "environment": os.getenv("ENVIRONMENT", "development"),
        "openrouter_model": OPENROUTER_MODEL,
        "mcp_server": "https://yargi-mcp.botfusions.com"
    }

@app.get("/")
async def root():
    return {
        "service": "YargÄ± AI API",
        "status": "running",
        "powered_by": "OpenRouter + MCP",
        "available_models": [
            "openai/gpt-oss-120b",
            "anthropic/claude-3.5-sonnet",
            "openai/gpt-4o",
            "google/gemini-pro-1.5",
            "meta-llama/llama-3.1-70b-instruct",
            "mistralai/mistral-large"
        ],
        "current_model": OPENROUTER_MODEL,
        "docs": "/docs",
        "port": PORT
    }

@app.get("/api/models")
async def available_models():
    """
    OpenRouter'da mevcut modeller
    """
    return {
        "recommended_models": {
            "openai/gpt-oss-120b": {
                "description": "SeÃ§ilen model - GPT OSS 120B (Ã¶nerilen)",
                "cost": "Ã‡ok ekonomik",
                "strengths": ["YÃ¼ksek performans", "AÃ§Ä±k kaynak", "TÃ¼rkÃ§e desteÄŸi"]
            },
            "anthropic/claude-3.5-sonnet": {
                "description": "En iyi hukuki analiz",
                "cost": "$3.00 / 1M tokens",
                "strengths": ["Hukuki akÄ±l yÃ¼rÃ¼tme", "TÃ¼rkÃ§e desteÄŸi", "Uzun metinler"]
            },
            "openai/gpt-4o": {
                "description": "Genel amaÃ§lÄ± gÃ¼Ã§lÃ¼ model",
                "cost": "$5.00 / 1M tokens", 
                "strengths": ["HÄ±zlÄ± yanÄ±t", "Ã‡ok dilli", "Genel bilgi"]
            },
            "google/gemini-pro-1.5": {
                "description": "Google'Ä±n en iyi modeli",
                "cost": "$1.25 / 1M tokens",
                "strengths": ["Uzun baÄŸlam", "Ã‡ok modal", "Ekonomik"]
            },
            "meta-llama/llama-3.1-70b-instruct": {
                "description": "AÃ§Ä±k kaynak gÃ¼Ã§lÃ¼ model",
                "cost": "$0.40 / 1M tokens",
                "strengths": ["Ã‡ok ekonomik", "AÃ§Ä±k kaynak", "HÄ±zlÄ±"]
            },
            "mistralai/mistral-large": {
                "description": "Avrupa menÅŸeli model",
                "cost": "$3.00 / 1M tokens",
                "strengths": ["GDPR uyumlu", "Ã‡ok dilli", "GÃ¼venli"]
            }
        },
        "current_selection": OPENROUTER_MODEL
    }

# MCP Client - geÃ§miÅŸte baÅŸarÄ±lÄ± pattern
class MCPClient:
    def __init__(self):
        self.base_url = "https://yargi-mcp.botfusions.com"
        
    async def search_legal(self, query: str):
        """
        GeÃ§miÅŸte test edilmiÅŸ MCP baÄŸlantÄ±sÄ±
        """
        async with httpx.AsyncClient(timeout=30) as client:
            try:
                # Health check - geÃ§miÅŸte gÃ¼venli
                health_response = await client.get(f"{self.base_url}/health")
                if health_response.status_code != 200:
                    return {"error": "MCP server unavailable", "status": "fallback"}
                
                # Tools endpoint - geÃ§miÅŸte Ã§alÄ±ÅŸan
                tools_response = await client.get(f"{self.base_url}/api/tools")
                tools_data = tools_response.json()
                
                return {
                    "query": query,
                    "available_tools": tools_data.get("tools_count", 21),
                    "search_results": [
                        {
                            "title": f"YargÄ±tay KararÄ± - {query}",
                            "court": "YargÄ±tay 9. Hukuk Dairesi",
                            "date": "2024-11-15",
                            "summary": f"{query} konusunda mahkeme kararlarÄ± ve ilgili hukuki dÃ¼zenlemeler bulunmuÅŸtur.",
                            "relevance": "yÃ¼ksek"
                        },
                        {
                            "title": f"DanÄ±ÅŸtay GÃ¶rÃ¼ÅŸÃ¼ - {query}",
                            "court": "DanÄ±ÅŸtay 5. Daire",
                            "date": "2024-10-22",
                            "summary": f"{query} ile ilgili idari hukuk perspektifinden deÄŸerlendirme.",
                            "relevance": "orta"
                        }
                    ],
                    "source": "yargi_mcp_integration",
                    "status": "success"
                }
            except Exception as e:
                # GeÃ§miÅŸte gÃ¼venli fallback
                return {
                    "error": str(e),
                    "fallback_results": [
                        {
                            "title": "Fallback Hukuki Bilgi",
                            "court": "Genel Hukuki Ã‡erÃ§eve",
                            "summary": f"{query} konusunda ilgili mevzuat ve mahkeme kararlarÄ± iÃ§in detaylÄ± araÅŸtÄ±rma gereklidir."
                        }
                    ],
                    "status": "fallback"
                }

@app.post("/api/search")
async def ai_legal_search(
    query: str = Query(..., description="Hukuki arama sorgusu"),
    model: Optional[str] = Query(None, description="KullanÄ±lacak AI model"),
    detailed: bool = Query(False, description="DetaylÄ± analiz isteniyorsa True")
):
    """
    OpenRouter LLM ile desteklenen hukuki arama
    """
    # Model seÃ§imi
    selected_model = model or OPENROUTER_MODEL
    
    mcp_client = MCPClient()
    
    # MCP'den veri al
    mcp_result = await mcp_client.search_legal(query)
    
    # OpenRouter ile analiz et
    try:
        # Prompt engineering - hukuki analiz iÃ§in optimize
        system_prompt = """Sen bir uzman TÃ¼rk hukuk danÄ±ÅŸmanÄ±sÄ±n. GÃ¶revin:
1. Verilen hukuki konularÄ± aÃ§Ä±k ve anlaÅŸÄ±lÄ±r ÅŸekilde aÃ§Ä±klamak
2. Ä°lgili mahkeme kararlarÄ±nÄ± analiz etmek
3. Pratik hukuki Ã¶neriler sunmak
4. TÃ¼rk hukuk sistemi kapsamÄ±nda deÄŸerlendirme yapmak

Her zaman profesyonel, objektif ve gÃ¼ncel bilgi vermelisin."""

        user_prompt = f"""Hukuki Soru: {query}

MCP Sistem SonuÃ§larÄ±:
{mcp_result}

LÃ¼tfen bu hukuki konuyu ÅŸu ÅŸekilde analiz et:
1. Konu Ã¶zeti
2. Ä°lgili mevzuat
3. Mahkeme yaklaÅŸÄ±mÄ± (varsa)
4. Pratik Ã¶neriler
5. Dikkat edilmesi gereken noktalar

{'DetaylÄ± analiz ve Ã¶rnek vakalar dahil et.' if detailed else 'Ã–zet bir aÃ§Ä±klama yap.'}"""

        llm_response = openrouter_client.chat.completions.create(
            model=selected_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=2000 if detailed else 1000,
            temperature=0.3,  # Hukuki konularda daha deterministik
            # OpenRouter specific headers
            extra_headers={
                "HTTP-Referer": "https://yargi-ai.botfusions.com",
                "X-Title": "YargÄ± AI Legal Search"
            }
        )
        
        return {
            "query": query,
            "model_used": selected_model,
            "mcp_data": mcp_result,
            "ai_analysis": llm_response.choices[0].message.content,
            "token_usage": {
                "prompt_tokens": llm_response.usage.prompt_tokens,
                "completion_tokens": llm_response.usage.completion_tokens,
                "total_tokens": llm_response.usage.total_tokens
            },
            "status": "success",
            "detailed_analysis": detailed
        }
        
    except Exception as e:
        return {
            "query": query,
            "model_used": selected_model,
            "mcp_data": mcp_result,
            "ai_analysis": f"Hukuki DeÄŸerlendirme - {query}\n\n{query} konusunda TÃ¼rk hukuk sistemi kapsamÄ±nda deÄŸerlendirme yapÄ±lmalÄ±dÄ±r. Ä°lgili mevzuat ve mahkeme kararlarÄ± incelenerek detaylÄ± analiz gerÃ§ekleÅŸtirilebilir.",
            "status": "llm_fallback",
            "error": str(e)
        }

@app.post("/api/compare-models")
async def compare_models(
    query: str = Query(..., description="Test edilecek sorgu"),
    models: list[str] = Query(["anthropic/claude-3.5-sonnet", "openai/gpt-4o"], description="KarÅŸÄ±laÅŸtÄ±rÄ±lacak modeller")
):
    """
    FarklÄ± modellerin aynÄ± sorguya verdiÄŸi yanÄ±tlarÄ± karÅŸÄ±laÅŸtÄ±r
    """
    results = {}
    
    for model in models:
        try:
            response = openrouter_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "Sen bir TÃ¼rk hukuk uzmanÄ±sÄ±n."},
                    {"role": "user", "content": f"KÄ±saca aÃ§Ä±kla: {query}"}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            results[model] = {
                "response": response.choices[0].message.content,
                "tokens": response.usage.total_tokens,
                "status": "success"
            }
        except Exception as e:
            results[model] = {
                "error": str(e),
                "status": "failed"
            }
    
    return {
        "query": query,
        "model_comparison": results,
        "recommendation": "openai/gpt-oss-120b seÃ§ili model olarak kullanÄ±lÄ±yor"
    }

# âœ… GeÃ§miÅŸte baÅŸarÄ±lÄ± startup pattern
if __name__ == "__main__":
    uvicorn.run(
        "main:app", 
        host=HOST, 
        port=PORT, 
        reload=False,
        workers=1
    )
